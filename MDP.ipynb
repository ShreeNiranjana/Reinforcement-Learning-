{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01347983",
   "metadata": {},
   "source": [
    "#### Problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af41e04",
   "metadata": {},
   "source": [
    "Create a 100x100 grid with obstacles in between 2 random points. \n",
    "Build an MDP based RL agent to optimise both policies and actions at every state.\n",
    "Benchmark DP method with other RL solutions for the same problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b5efc",
   "metadata": {},
   "source": [
    "#### Algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372b604",
   "metadata": {},
   "source": [
    "Define the Grid and Obstacles: Create a 100x100 grid with randomly placed obstacles.\n",
    "\n",
    "Define MDP (Markov Decision Process): Define states, actions, rewards, and transitions for this environment.\n",
    "\n",
    "Dynamic Programming Solution: Implement a Dynamic Programming approach for policy optimization.\n",
    "\n",
    "Q-Learning Agent: Implement a Q-learning agent as a comparison for the DP solution.\n",
    "\n",
    "Benchmark and Comparison: Evaluate the agents' performances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e5d21e",
   "metadata": {},
   "source": [
    "#### Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "283e1c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2d19eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SIZE = 100\n",
    "OBSTACLE_PROBABILITY = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c850f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "ACTION_MAP = {0: (0, 1), 1: (0, -1), 2: (1, 0), 3: (-1, 0)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fb0972",
   "metadata": {},
   "source": [
    "###### Dynamic Programming Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "498977af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridMDP:\n",
    "    def __init__(self, grid_size, obstacle_prob):\n",
    "        self.grid_size = grid_size\n",
    "        self.obstacle_prob = obstacle_prob\n",
    "        self.grid = self.create_grid()\n",
    "        self.start, self.goal = self.get_start_goal()\n",
    "\n",
    "    def create_grid(self):\n",
    "        grid = np.zeros((self.grid_size, self.grid_size), dtype=int)\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                if random.random() < self.obstacle_prob:\n",
    "                    grid[i, j] = -1  #obstacle\n",
    "        return grid\n",
    "\n",
    "    def get_start_goal(self):\n",
    "        while True:\n",
    "            start = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))\n",
    "            goal = (random.randint(0, self.grid_size - 1), random.randint(0, self.grid_size - 1))\n",
    "            if start != goal and self.grid[start] == 0 and self.grid[goal] == 0:\n",
    "                return start, goal\n",
    "\n",
    "    def is_valid(self, pos):\n",
    "        x, y = pos\n",
    "        return 0 <= x < self.grid_size and 0 <= y < self.grid_size and self.grid[x, y] == 0\n",
    "\n",
    "    def step(self, state, action):\n",
    "        next_state = (state[0] + action[0], state[1] + action[1])\n",
    "        if not self.is_valid(next_state):\n",
    "            return state, -1  #Penalty \n",
    "        if next_state == self.goal:\n",
    "            return next_state, 100  #reward \n",
    "        return next_state, -1  #step cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6243c3a",
   "metadata": {},
   "source": [
    "###### Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13c8aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1, episodes=5000):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.episodes = episodes\n",
    "        self.q_table = defaultdict(lambda: np.zeros(len(ACTIONS)))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(ACTIONS)\n",
    "        state_index = tuple(state)\n",
    "        return ACTIONS[np.argmax(self.q_table[state_index])]\n",
    "\n",
    "    def learn(self):\n",
    "        for episode in range(self.episodes):\n",
    "            state = self.env.start\n",
    "            while state != self.env.goal:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward = self.env.step(state, action)\n",
    "                state_index = tuple(state)\n",
    "                next_index = tuple(next_state)\n",
    "                best_next_action = np.max(self.q_table[next_index])\n",
    "                self.q_table[state_index][ACTIONS.index(action)] += self.alpha * (\n",
    "                    reward + self.gamma * best_next_action - self.q_table[state_index][ACTIONS.index(action)]\n",
    "                )\n",
    "                state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0faa2190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking\n",
    "env = GridMDP(GRID_SIZE, OBSTACLE_PROBABILITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d32b3cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic programming agent\n",
    "dp_agent = DPAgent(env)\n",
    "dp_agent.policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acfb1220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning agent\n",
    "q_agent = QLearningAgent(env)\n",
    "q_agent.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a4c34",
   "metadata": {},
   "source": [
    "##### Comparing policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21e6c0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP Success Rate: 1.0, DP Avg Steps: 22.0\n",
      "Q-Learning Success Rate: 1.0, Q-Learning Avg Steps: 25.08\n"
     ]
    }
   ],
   "source": [
    "def evaluate_agent(agent, episodes=100):\n",
    "    success_count = 0\n",
    "    total_steps = 0\n",
    "    for _ in range(episodes):\n",
    "        state = env.start\n",
    "        steps = 0\n",
    "        while state != env.goal:\n",
    "            action = ACTIONS[agent.policy[state]] if isinstance(agent, DPAgent) else agent.choose_action(state)\n",
    "            next_state, _ = env.step(state, action)\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            if steps > 10000:  \n",
    "                break\n",
    "        if state == env.goal:\n",
    "            success_count += 1\n",
    "        total_steps += steps\n",
    "    success_rate = success_count / episodes\n",
    "    avg_steps = total_steps / episodes\n",
    "    return success_rate, avg_steps\n",
    "\n",
    "dp_success_rate, dp_avg_steps = evaluate_agent(dp_agent)\n",
    "q_success_rate, q_avg_steps = evaluate_agent(q_agent)\n",
    "\n",
    "print(f\"DP Success Rate: {dp_success_rate}, DP Avg Steps: {dp_avg_steps}\")\n",
    "print(f\"Q-Learning Success Rate: {q_success_rate}, Q-Learning Avg Steps: {q_avg_steps}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
